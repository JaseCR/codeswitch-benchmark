{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test OpenAI Adapter\n",
    "\n",
    "This notebook tests the OpenAI adapter with different configurations for code-switching analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI key: True\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports and environment\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Import our adapter\n",
    "from adapters.openai_adapter import OpenAIAdapter\n",
    "\n",
    "# Verify that your key loaded correctly\n",
    "print(\"OpenAI key:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI adapter initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI Pro adapter\n",
    "print(\"üöÄ Setting up OpenAI Pro adapter...\")\n",
    "print(\"‚úÖ You have both Cursor Pro + OpenAI Pro - full access!\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# With OpenAI Pro, try the best models first\n",
    "models_to_try = [\"gpt-4o\", \"gpt-4-turbo\", \"gpt-4o-mini\", \"gpt-3.5-turbo\"]\n",
    "\n",
    "working_model = None\n",
    "for model in models_to_try:\n",
    "    try:\n",
    "        print(f\"\\nüß™ Testing {model}...\")\n",
    "        test_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        test_response = test_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        print(f\"‚úÖ {model} works!\")\n",
    "        working_model = model\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {model} failed: {e}\")\n",
    "\n",
    "if working_model:\n",
    "    print(f\"\\nüéâ Using {working_model} for the adapter\")\n",
    "    # Use higher limits with Pro access\n",
    "    max_tokens = 1000 if \"gpt-4\" in working_model else 500\n",
    "    openai_adapter = OpenAIAdapter(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model=working_model,\n",
    "        temperature=0.7,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    print(f\"‚úÖ OpenAI Pro adapter initialized with {working_model}\")\n",
    "    print(f\"üìä Max tokens: {max_tokens}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No working models found. This shouldn't happen with Pro access!\")\n",
    "    print(\"üí° Check your API key and billing status\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ OpenAI Pro Access\n",
    "\n",
    "**Perfect Setup**: You have both Cursor Pro + OpenAI Pro!\n",
    "\n",
    "### **Your Access:**\n",
    "- ‚úÖ **Cursor Pro**: Enhanced IDE features\n",
    "- ‚úÖ **OpenAI Pro**: Full API access with higher limits\n",
    "\n",
    "### **OpenAI Pro Benefits:**\n",
    "- **GPT-4o**: Latest and most capable model\n",
    "- **GPT-4-turbo**: Fast and efficient\n",
    "- **Higher rate limits**: More requests per minute\n",
    "- **Higher token limits**: Longer responses\n",
    "- **Priority access**: Better availability\n",
    "\n",
    "### **What You Can Do:**\n",
    "- Use the best models (GPT-4o, GPT-4-turbo)\n",
    "- Process larger datasets\n",
    "- Generate longer responses\n",
    "- No quota concerns for normal usage\n",
    "\n",
    "### **Ready to Go:**\n",
    "The notebook will automatically use your best available model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating response: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response: None\n"
     ]
    }
   ],
   "source": [
    "# Test basic generation\n",
    "test_prompt = \"Paraphrase this sentence without changing its meaning: He finna go to the store.\"\n",
    "\n",
    "response = openai_adapter.generate_response(test_prompt)\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with system prompt for better code-switching analysis\n",
    "system_prompt = \"\"\"You are a linguist analyzing code-switching behavior. \n",
    "Your task is to preserve the dialectal style and cultural context while \n",
    "paraphrasing or continuing text. Maintain the same linguistic variety \n",
    "and register as the input.\"\"\"\n",
    "\n",
    "user_prompt = \"Paraphrase this AAVE sentence: He finna go to the store. You sliding?\"\n",
    "\n",
    "response = openai_adapter.generate_with_system_prompt(system_prompt, user_prompt)\n",
    "print(\"Response with system prompt:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple candidates\n",
    "candidates = openai_adapter.generate_multiple_candidates(\n",
    "    \"Continue this Spanglish sentence: Vamos later, it's muy close to la tienda.\",\n",
    "    num_candidates=3\n",
    ")\n",
    "\n",
    "print(\"Multiple candidates:\")\n",
    "for i, candidate in enumerate(candidates, 1):\n",
    "    print(f\"{i}. {candidate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperature settings\n",
    "temperatures = [0.2, 0.5, 0.8]\n",
    "prompt = \"Explain this British English phrase: 'We're off on holiday next week, fancy it?'\"\n",
    "\n",
    "print(\"Testing different temperature settings:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for temp in temperatures:\n",
    "    openai_adapter.update_config(temperature=temp)\n",
    "    response = openai_adapter.generate_response(prompt)\n",
    "    print(f\"\\nüå°Ô∏è Temperature {temp}:\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with retry mechanism\n",
    "print(\"Testing retry mechanism:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "response = openai_adapter.generate_with_retry(\n",
    "    \"Paraphrase this: Ion think that plan gon' work.\",\n",
    "    max_retries=3\n",
    ")\n",
    "print(f\"Response with retry: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available models\n",
    "print(\"Available OpenAI models:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "models = openai_adapter.get_available_models()\n",
    "for model in models:\n",
    "    print(f\"‚Ä¢ {model}\")\n",
    "\n",
    "print(f\"\\nTotal models available: {len(models)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different Pro models\n",
    "print(\"Testing different OpenAI Pro models:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "models_to_test = [\"gpt-4o\", \"gpt-4o-mini\", \"gpt-4-turbo\", \"o1-preview\"]\n",
    "test_prompt = \"Continue this sentence in the same style: We was tryna finish that yesterday\"\n",
    "\n",
    "for model in models_to_test:\n",
    "    try:\n",
    "        print(f\"\\nü§ñ Testing {model}:\")\n",
    "        adapter = OpenAIAdapter(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            model=model,\n",
    "            temperature=0.7,\n",
    "            max_tokens=300  # Higher limits with Pro\n",
    "        )\n",
    "        response = adapter.generate_response(test_prompt)\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 40)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {model}: {e}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch processing with our stimuli data\n",
    "print(\"Loading stimuli data for batch processing:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "stimuli = pd.read_csv(\"../data/raw/stimuli.csv\")\n",
    "print(f\"Loaded {len(stimuli)} stimuli\")\n",
    "print(\"\\nFirst few examples:\")\n",
    "stimuli.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process examples with OpenAI Pro (higher rate limits)\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Processing stimuli with OpenAI Pro:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Reset to good settings for batch processing with Pro limits\n",
    "openai_adapter.update_config(temperature=0.5, max_tokens=500)\n",
    "\n",
    "responses = []\n",
    "for i, row in tqdm(stimuli.head(5).iterrows(), total=5):  # Process more with Pro\n",
    "    # Create task-specific prompt\n",
    "    if row.task == \"paraphrase\":\n",
    "        prompt = f\"Paraphrase this text in the same dialectal style: {row.text}\"\n",
    "    elif row.task == \"explain\":\n",
    "        prompt = f\"Explain this text in simple terms while preserving the dialectal style: {row.text}\"\n",
    "    elif row.task == \"continue\":\n",
    "        prompt = f\"Continue this text in the same dialectal style: {row.text}\"\n",
    "    else:\n",
    "        prompt = f\"Process this text while maintaining the dialectal style: {row.text}\"\n",
    "    \n",
    "    output = openai_adapter.generate_response(prompt)\n",
    "    responses.append({\n",
    "        \"id\": row.id,\n",
    "        \"variety\": row.variety,\n",
    "        \"task\": row.task,\n",
    "        \"input_text\": row.text,\n",
    "        \"output_text\": output\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(responses)\n",
    "print(\"\\nOpenAI Pro responses:\")\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
